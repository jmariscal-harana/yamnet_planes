{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuned YAMNet: small plane detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio, librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "# Decide what type of messages are displayed by TensorFlow (ERROR, WARN, INFO, DEBUG, FATAL)\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tf version:  1.15.0\ntf.keras version:  2.2.4-tf\n"
    }
   ],
   "source": [
    "## TensorFlow memory allocation options:\n",
    "# OPTION 1: \"smart\" allocation\n",
    "config=tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess=tf.Session(config=config) \n",
    "\n",
    "# OPTION 2: maximum memory allocation per session (0-1 = 0-100%)\n",
    "# gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.05)\n",
    "# config = tf.ConfigProto(gpu_options=gpu_options)\n",
    "# sess=tf.Session(config=config)\n",
    "\n",
    "# OPTION 3: ???\n",
    "#physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "#tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "# OPTION 4: ???\n",
    "#Slim/embedded versions: (https://github.com/google-research/tf-slim)\n",
    "\n",
    "print(\"tf version: \", tf.__version__)\n",
    "print(\"tf.keras version: \", tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add/append required paths\n",
    "import os, sys\n",
    "\n",
    "path_root = '/home/ups/Proyectos/Vigia_sonido/Models/yamnet_planes/' #path to main folder\n",
    "# path_root = input(\"Enter the path of your repository: \") # ask user for path_root\n",
    "assert os.path.exists(path_root)\n",
    "sys.path.append(path_root)\n",
    "\n",
    "path_yamnet_original = path_root+'/yamnet_original/' #path to original yamnet files\n",
    "assert os.path.exists(path_yamnet_original)\n",
    "sys.path.append(path_yamnet_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YAMNet (original) is only used to extract class names\n",
    "import yamnet as yamnet_original\n",
    "\n",
    "class_names = yamnet_original.class_names(path_yamnet_original+'yamnet_class_map.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "## Modified YAMNet model for feature extraction\n",
    "import modified_yamnet as yamnet_modified\n",
    "import params\n",
    "\n",
    "params.PATCH_HOP_SECONDS = 0.048 #low values: higher accuracy but higher computational cost\n",
    "\n",
    "yamnet_features = yamnet_modified.yamnet_frames_model(params)\n",
    "yamnet_features.load_weights(path_root+'yamnet.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEMP\n",
    "# Load waveform\n",
    "DESIRED_SR = params.SAMPLE_RATE # required by YAMNet\n",
    "\n",
    "file_name = \"/home/ups/Proyectos/Vigia_sonido/Datasets/airplanes_v0/training_data/plane/track_04_0000_norm.wav\"\n",
    "waveform = read_wav(file_name, DESIRED_SR, use_rosa=1)\n",
    "waveform = waveform[0:4*DESIRED_SR]\n",
    "\n",
    "print('Waveform size:', waveform.shape, 'and duration:', waveform.shape[0]/DESIRED_SR, '[s]')\n",
    "\n",
    "# Calculate scores and spectrogram\n",
    "spectrogram, _, _, scores = yamnet_features.predict(np.reshape(waveform, [1, -1]), steps=1)\n",
    "\n",
    "print('Spectrogram size:',spectrogram.shape)\n",
    "print('Scores size:',scores.shape)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results.\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Plot the waveform.\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(waveform)\n",
    "plt.xlim([0, len(waveform)])\n",
    "# Plot the log-mel spectrogram (returned by the model).\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.imshow(spectrogram.T, aspect='auto', interpolation='nearest', origin='bottom')\n",
    "\n",
    "# Plot and label the model output scores for the top-scoring classes.\n",
    "mean_scores = np.mean(scores, axis=0)\n",
    "top_N = 10\n",
    "top_class_indices = np.argsort(mean_scores)[::-1][:top_N]\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.imshow(scores[:, top_class_indices].T, aspect='auto', interpolation='nearest', cmap='gray_r')\n",
    "# Compensate for the PATCH_WINDOW_SECONDS (0.96 s) context window to align with spectrogram.\n",
    "patch_padding = (params.PATCH_WINDOW_SECONDS / 2) / params.PATCH_HOP_SECONDS\n",
    "plt.xlim([-patch_padding, scores.shape[0] + patch_padding])\n",
    "# Label the top_N classes.\n",
    "yticks = range(0, top_N, 1)\n",
    "plt.yticks(yticks, [class_names[top_class_indices[x]] for x in yticks])\n",
    "_ = plt.ylim(-0.5 + np.array([top_N, 0]))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_wav(fname, output_sr, use_rosa=True):\n",
    "    # small wrapper - i was seeing some slightly different \n",
    "    # results when loading with different libraries \n",
    "    if use_rosa:\n",
    "        waveform, sr = librosa.load(fname, sr=output_sr)\n",
    "    else:\n",
    "        wav_data, sr = sf.read(fname, dtype=np.int16)\n",
    "        \n",
    "        if wav_data.ndim > 1: \n",
    "            # (ns, 2)\n",
    "            wav_data = wav_data.mean(1)\n",
    "        if sr != output_sr:\n",
    "            wav_data = resampy.resample(wav_data, sr, output_sr)\n",
    "        waveform = wav_data / 32768.0\n",
    "    \n",
    "    return waveform.astype(np.float64)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_silence(waveform, top_db=15, min_chunk_size=2000, merge_chunks=True):\n",
    "    # Loads sample into chunks of non-silence \n",
    "    \n",
    "    splits = librosa.effects.split(waveform, top_db=top_db)\n",
    "    \n",
    "    waves = []\n",
    "    for start, end in splits:\n",
    "        if (end-start) < min_chunk_size:\n",
    "            continue\n",
    "        waves.append(waveform[start:end])\n",
    "    \n",
    "    if merge_chunks:\n",
    "        out = None\n",
    "        for c in waves:\n",
    "            if out is None:\n",
    "                out = c.copy()\n",
    "            else:\n",
    "                out = np.concatenate((out, c))\n",
    "        waves = out\n",
    "    \n",
    "    return waves\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_dirs(p):\n",
    "    dirs = list(filter(lambda x : os.path.isdir( os.path.join(p, x) ), os.listdir(p)))\n",
    "    return list(map(lambda x : os.path.join(p, x), dirs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_augment_wav(wav_data):\n",
    "    # apply some random augmentations to the sound\n",
    "    # - time stretch, resample, volume change, minor noise \n",
    "    # - this has not been evaluated to measure contributions\n",
    "    # - TODO: probably a lot more augmentations you could use \n",
    "    \n",
    "    wav_data = wav_data.copy() \n",
    "    \n",
    "    # random re-sample \n",
    "    if np.random.uniform() > 0.8:\n",
    "        stretch = np.random.uniform(0.75, 1.5)\n",
    "        wav_data = librosa.effects.time_stretch(wav_data, stretch)\n",
    "    elif np.random.uniform() > 0.2:\n",
    "        new_sr = int(DESIRED_SR * np.random.uniform(0.9, 1.1))\n",
    "        wav_data = resampy.resample(wav_data, DESIRED_SR, new_sr)\n",
    "    \n",
    "    #librosa.effects.pitch_shift()\n",
    "    \n",
    "    # random volume\n",
    "    volume = np.random.uniform(0.65, 1.2)\n",
    "    wav_data = wav_data * volume\n",
    "    \n",
    "    # Random noise\n",
    "    if np.random.uniform() > 0.5:\n",
    "        NR = 0.001 # 0.1\n",
    "        wav_data += np.random.uniform(-NR, NR, size=wav_data.shape)\n",
    "    \n",
    "    return wav_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, resampy\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_data(data_path, \n",
    "              yamnet_features, \n",
    "              num_augmentations=5,\n",
    "              max_sample_seconds=5.0,\n",
    "              use_rosa=True):\n",
    "    \"\"\"\n",
    "    Loads data from .wav files contained in subfolders where \n",
    "    folder name is label, then runs them \n",
    "    through the audio_model to get feature vectors \n",
    "    and returns:\n",
    "    \n",
    "    X : [ np.array(1024) , ... ]\n",
    "    Y : [ category_idx , ...]\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    label_dirs = get_top_dirs(data_path)\n",
    "\n",
    "    _samples = []\n",
    "    _labels = []\n",
    "    \n",
    "    merge_chunks = True\n",
    "    MIN_WAV_SIZE = 5000 #\n",
    "    max_wav_size = int(DESIRED_SR * max_sample_seconds)\n",
    "    \n",
    "    for label_idx, label_dir in enumerate(label_dirs):\n",
    "        \n",
    "        label_name = os.path.basename(label_dir)\n",
    "        wavs = glob.glob(os.path.join(label_dir, \"*.wav\"))\n",
    "        print(\" Loading {:<5} '{:<40}'\".format(label_idx, label_name))\n",
    "\n",
    "        for wav_file in tqdm(wavs):\n",
    "            \n",
    "            # rosa seems very different?\n",
    "            #for use_rosa in range(2):\n",
    "            if True:\n",
    "                #use_rosa = 1\n",
    "                #use_rosa = np.random.uniform() > 0.5\n",
    "                waveform = read_wav(wav_file, DESIRED_SR, use_rosa=use_rosa)\n",
    "\n",
    "                if len(waveform) < MIN_WAV_SIZE:\n",
    "                    continue \n",
    "\n",
    "                if len(waveform) > max_wav_size:\n",
    "                    waveform = waveform[:max_wav_size]\n",
    "                    print(\"\\nIgnoring audio data after {} seconds\".format(max_sample_seconds))\n",
    "\n",
    "                for aug_idx in range(1 + num_augmentations):\n",
    "                    \n",
    "                    aug_wav = waveform.copy()\n",
    "                    \n",
    "                    if aug_idx > 0:\n",
    "                        aug_wav = random_augment_wav(aug_wav)\n",
    "\n",
    "                    _, _, dense_out, _ = yamnet_features.predict(np.reshape(aug_wav, [1, -1]), steps=1)\n",
    "                    \n",
    "                    for patch in dense_out:\n",
    "                        _samples.append(patch)\n",
    "                        _labels.append(label_idx)\n",
    "                \n",
    "    return _samples, _labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_models(waveform, \n",
    "               yamnet_features, \n",
    "               top_model, \n",
    "               strip_silence=False, \n",
    "               min_samples=16000):\n",
    "    \n",
    "    if strip_silence:\n",
    "        waveform = remove_silence(waveform, top_db=10)\n",
    "    \n",
    "    if len(waveform) < min_samples:\n",
    "        print(\"input too short after silence removal\")\n",
    "        return [-1] #this value will be used to discard this audio later\n",
    "    \n",
    "    _, _, dense_out, _ = yamnet_features.predict(np.reshape(waveform, [1, -1]), steps=1)\n",
    "    \n",
    "    # dense = (N, 1024)\n",
    "    all_scores = []\n",
    "    for patch in dense_out:\n",
    "        scores = top_model.predict(np.expand_dims(patch,0)).squeeze()\n",
    "        all_scores.append(scores)\n",
    "        \n",
    "    all_scores = np.mean(all_scores, axis=0)\n",
    "    return all_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YAMNet (original) predictions on a single audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load waveform\n",
    "DESIRED_SR = params.SAMPLE_RATE # required by YAMNet\n",
    "\n",
    "file_name = \"/home/ups/Proyectos/Vigia_sonido/Datasets/airplanes_v0/training_data/plane/track_04_0000_norm.wav\"\n",
    "waveform = read_wav(file_name, DESIRED_SR, use_rosa=1)\n",
    "\n",
    "# Calculate scores and spectrogram\n",
    "spectrogram, _, _, scores = yamnet_features.predict(np.reshape(waveform, [1, -1]), steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results.\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Plot the waveform.\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(waveform)\n",
    "plt.xlim([0, len(waveform)])\n",
    "# Plot the log-mel spectrogram (returned by the model).\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.imshow(spectrogram.T, aspect='auto', interpolation='nearest', origin='bottom')\n",
    "\n",
    "# Plot and label the model output scores for the top-scoring classes.\n",
    "mean_scores = np.mean(scores, axis=0)\n",
    "top_N = 10\n",
    "top_class_indices = np.argsort(mean_scores)[::-1][:top_N]\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.imshow(scores[:, top_class_indices].T, aspect='auto', interpolation='nearest', cmap='gray_r')\n",
    "# Compensate for the PATCH_WINDOW_SECONDS (0.96 s) context window to align with spectrogram.\n",
    "patch_padding = (params.PATCH_WINDOW_SECONDS / 2) / params.PATCH_HOP_SECONDS\n",
    "plt.xlim([-patch_padding, scores.shape[0] + patch_padding])\n",
    "# Label the top_N classes.\n",
    "yticks = range(0, top_N, 1)\n",
    "plt.yticks(yticks, [class_names[top_class_indices[x]] for x in yticks])\n",
    "_ = plt.ylim(-0.5 + np.array([top_N, 0]))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "0%|          | 0/16 [00:00<?, ?it/s] Loading 0     'not_plane                               '\n100%|██████████| 16/16 [00:03<00:00,  4.14it/s]\n  7%|▋         | 1/14 [00:00<00:02,  6.12it/s] Loading 1     'plane                                   '\n100%|██████████| 14/14 [00:02<00:00,  4.88it/s]\n"
    }
   ],
   "source": [
    "# samples, labels = load_data(\"/home/ups/Proyectos/Vigia_sonido/Datasets/airplanes_v0/training_data/\",\n",
    "#                             yamnet_features,\n",
    "#                             num_augmentations=4, \n",
    "#                             max_sample_seconds=5.0)\n",
    "\n",
    "samples, labels = load_data(\"/home/ups/Proyectos/Vigia_sonido/Datasets/airplanes_v0/training_data/\",\n",
    "                            yamnet_features,\n",
    "                            num_augmentations=1, \n",
    "                            max_sample_seconds=5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "idxs = list(range(len(labels)))\n",
    "random.shuffle(idxs)\n",
    "\n",
    "samples = [samples[i] for i in idxs]\n",
    "labels = [labels[i] for i in idxs]\n",
    "\n",
    "samples = np.array(samples)\n",
    "labels = np.array(labels)\n",
    "\n",
    "print(\" Loaded samples: \" , samples.shape, samples.dtype,  labels.shape)\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Classifier which takes YAMNet's feature vector as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model, layers\n",
    "\n",
    "def yamnet_classifier(input_size=1024,\n",
    "                  num_hidden=1024,\n",
    "                  num_classes=2):\n",
    "\n",
    "    input_layer = layers.Input(shape=(input_size,))    \n",
    "    dense_layer = layers.Dense(num_hidden, activation=None)(input_layer)\n",
    "    classifier_layer = layers.Dense(num_classes, activation='softmax')(dense_layer)\n",
    "    model = Model(inputs=input_layer, outputs=classifier_layer)\n",
    "\n",
    "    return model\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n",
    "save_best = ModelCheckpoint('top_model.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "# Classifier definition\n",
    "feature_vector_length = 1024\n",
    "num_hidden = 1024\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "yamnet_planes = yamnet_classifier(input_size=feature_vector_length, \n",
    "                      num_hidden=num_hidden,\n",
    "                      num_classes=NUM_CLASSES)\n",
    "\n",
    "#opt = Adam(learning_rate=0.001)\n",
    "opt = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "yamnet_planes.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# Train the model \n",
    "history = yamnet_planes.fit(samples, labels, epochs=200, validation_split=0.1)\n",
    "# history = yamnet_planes.fit(samples, labels, epochs=200, validation_split=0.1, callbacks=[save_best])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "yamnet_planes = load_model(path_root+'top_model_v2.h5')\n",
    "opt = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "yamnet_planes.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "yamnet_planes.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test on holdout sounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Specify class labels\n",
    "class_labels = [\"not plane\", \"plane\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scores for a single holdout audio\n",
    "file_name = \"/home/ups/Proyectos/Vigia_sonido/Datasets/airplanes_v2/holdout_data/plane/track_04_0000_norm.wav\"\n",
    "waveform = read_wav(file_name, DESIRED_SR, use_rosa=1)\n",
    "\n",
    "scores = run_models(waveform, yamnet_features, yamnet_planes)\n",
    "winner = class_labels[scores.argmax()]\n",
    "\n",
    "print(\" Best score: {}  label: {}\".format(scores.max(), winner))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scores for a holdout folder\n",
    "holdout_dir = \"/home/ups/Proyectos/Vigia_sonido/Datasets/airplanes_v2/holdout_data/plane/\"\n",
    "arr = os.listdir(holdout_dir)\n",
    "\n",
    "for fname in arr: \n",
    "    fname = holdout_dir+fname\n",
    "    waveform = read_wav(fname, DESIRED_SR, use_rosa=1)\n",
    "\n",
    "    # make file a bit longer by duplicating it \n",
    "    waveform = np.concatenate((waveform,waveform,waveform))\n",
    "\n",
    "    scores = run_models(waveform, yamnet_features, yamnet_planes, strip_silence=False)\n",
    "    winner_save = np.empty((0,2))\n",
    "    if scores[0] != -1:\n",
    "        winner = categories[scores.argmax()]\n",
    "        print(\" Best score: {}  label: {}\".format(scores.max(), winner))\n",
    "        # winner_save = np.append(winner_save,np.array([scores.max(),winner]),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_array = np.empty((0, 2))\n",
    "# result = np.array([1,\"a\"])\n",
    "# result_array = np.append(result_array, [result], axis=0)\n",
    "# result_array = np.append(result_array, [result], axis=0)\n",
    "\n",
    "# result_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the model for later\n",
    "model.save(\"top_model_v2.h5\", include_optimizer=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('tf_gpu': conda)",
   "language": "python",
   "name": "python37564bittfgpucondaef4f1cafeaaa426ca3b3af329183c059"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}